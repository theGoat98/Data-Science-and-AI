# Codegrade Tag Question3
# Do *not* remove the tag above
import numpy as np

class NaiveBinaryBayes:
    
    def __init__(self, alpha = 1.0):
        alpha = float(alpha)
        self.alpha = alpha
        pass
    

    
    def fit(self,X,y):
        n_features_in_ = X.shape[1]
        classes_ = np.unique(y)
        class_count_ = np.array([np.sum(y == c) for c in classes_])
        class_log_prior_ = np.log(class_count_ / y.size)
        category_count_ = []
        feature_log_prob_ = []
        for feature_index in range(n_features_in_):
            feature_values = X[:, feature_index]
            category_count = np.zeros((2, 2))  # 2 classes, 2 feature values (0 and 1)
            for class_index, class_label in enumerate(classes_):
                for feature_value in [0, 1]:
                    count = np.sum((y == class_label) & (feature_values == feature_value))
                    category_count[class_index, feature_value] = count + self.alpha  # Laplace smoothing
            category_count_.append(category_count)
            feature_log_prob = np.log(category_count / class_count_[:, np.newaxis])
            feature_log_prob_.append(feature_log_prob)
        self.n_features_in_ = n_features_in_
        self.classes_ = classes_
        self.class_count_ = class_count_
        self.class_log_prior_ = class_log_prior_    
        self.category_count_ = category_count_
        self.feature_log_prob_ = feature_log_prob_

        return self
        

    
    def predict_log_proba(self, X):
        #print(self.feature_log_prob_)
        
        log_proba = np.zeros((X.shape[0], 2))  # 2 classes
        for i in range(X.shape[0]): # for each observation
            log_probs = self.class_log_prior_.copy() # start with log priors
            for feature_index in range(self.n_features_in_): # for each feature
                feature_value = X[i, feature_index] # get feature value (0 or 1)
                log_probs += self.feature_log_prob_[feature_index][:,feature_value] # add log prob for this feature
           
            log_proba[i] = log_probs
        

        """
        Given an m*d array X, returns an m*2 array of log probabilities corresponding to the posterior probability of the observation coming from a given class

        Parameters:
        - X: an m*d array of observations to predict

        Return value:
        - An m*2 array of log probabilities
        """
        assert X.ndim == 2 and X.shape[1] == self.n_features_in_
        return log_proba

    
    def predict_proba(self, X):
        predict_proba = np.zeros((X.shape[0], 2))  # 2 classes
        log_probs = self.predict_log_proba(X)
        for i in range(X.shape[0]):
            max_log_prob = np.max(log_probs[i])
            probs = np.exp(log_probs[i] - max_log_prob)
            probs /= np.sum(probs)
            predict_proba[i] = probs
        return predict_proba
        """
        Given an m*d array X, returns an m*2 array of probabilities corresponding to the posterior probability of the observation coming from a given class

        Parameters:
        - X: an m*d array of observations to predict

        Return value:
        - An m*2 array of probabilities
        """
        assert X.ndim == 2 and X.shape[1] == self.n_features_in_
        raise NotImplementedError

    
    
    def predict(self, X):
        predict = np.zeros(X.shape[0], dtype=int)
        probs = self.predict_proba(X)
        for i in range(X.shape[0]):
            predict[i] = np.argmax(probs[i])
        return predict
        """
        Given an m*d array X, returns an m vector of predicted class labels

        Parameters:
        - X: an m*d array of observations to predict

        Return value:
        - An m array of class labels (corresponding to the maximum probability for each observation)
        """
        assert X.ndim == 2 and X.shape[1] == self.n_features_in_
        raise NotImplementedError
    


y = np.array([0, 0, 1, 1, 0, 1, 1, 1, 1, 0])
X = np.array([0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
       1, 0, 1, 1, 0, 1, 1, 0]).reshape(10,3)

nbb = NaiveBinaryBayes(alpha=0.0).fit(X,y)
X2 = np.array([1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
               0, 1, 0, 0, 0, 0, 1, 0]).reshape(10,3)
print('\n'.join(' '.join(l) for l in np.vectorize(lambda x: f'{x:+0.3f}')(nbb.predict_log_proba(X2)).tolist()))
print(X2)

nbb = NaiveBinaryBayes(alpha=0.0).fit(X_train,y_train)
from sklearn.naive_bayes import CategoricalNB
cnb = CategoricalNB(alpha=0.0).fit(X_train,y_train)
lp1 = nbb.predict_log_proba(X_test)
lp2 = cnb.predict_log_proba(X_test)


print('Log probs almost equal?',np.allclose(lp1,lp2))
