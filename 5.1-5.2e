# Codegrade Tag Question1
# Do *not* remove the tag above

from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)
val_loader = DataLoader(val_subset, batch_size=64, shuffle=False)
test_loader = DataLoader(test_set, batch_size=64, shuffle=False)

iter_train = iter(train_loader)
images, labels = next(iter_train)
batch_labels = labels  

fig, axs = plt.subplots(2, 5, figsize=(10, 5))
axs = axs.ravel()
for i in range(10):
    axs[i].imshow(images[i].squeeze(), cmap='gray')
    axs[i].set_title(f'Label: {batch_labels[i].item()}', va = "bottom")
    axs[i].axis('off')
plt.tight_layout()
plt.show()

# Codegrade Tag Question2a
# Do *not* remove the tag above

import torch.nn as nn
import torch

class SimpleNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super().__init__()

        # Build the network as required
        self.network = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, output_size)
        )

    def forward(self, x):
        x = x.view(x.size(0), -1)
        return self.network(x)
    
simple_nn = SimpleNN(input_size=784, hidden_size=64, output_size=10)


# Codegrade Tag Question2b
# Do *not* remove the tag above

import torch.optim as optim

def train_one_epoch(model, loader, loss_fn, optimizer):
    model.train() 
    losses = []
    correct = 0
    total = 0
    for data, target in loader: 
        optimizer.zero_grad()
        output = model(data)
        loss = loss_fn(output, target)
        loss.backward()
        optimizer.step()
        
        losses.append(loss.item())  
        _, predicted = torch.max(output, 1)
        total += target.size(0)
        correct += (predicted == target).sum().item()   
    mean_loss = sum(losses) / len(losses)
    accuracy = correct / total

    return mean_loss, accuracy

# Codegrade Tag Question2c
# Do *not* remove the tag above

def evaluate(model, loader, loss_fn):
    model.eval()
    losses = []
    correct = 0
    total = 0
    for data, target in loader: 
        torch.no_grad()
        output = model(data)
        loss = loss_fn(output, target)
        
        losses.append(loss.item())  
        _, predicted = torch.max(output, 1)
        total += target.size(0)
        correct += (predicted == target).sum().item()   
    mean_loss = sum(losses) / len(losses)
    accuracy = correct / total
    return mean_loss, accuracy

# Codegrade Tag Question2d
# Do *not* remove the tag above

def train(model, train_loader, val_loader, num_epochs, learning_rate = 0.0001):
    model.train()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    loss_fn = nn.CrossEntropyLoss()
    train_losses = []
    train_accuracies = []
    val_losses = []
    val_accuracies = []

    for epoch in range(num_epochs):
        train_loss, train_acc = train_one_epoch(model, train_loader, loss_fn, optimizer)
        train_losses.append(train_loss)
        train_accuracies.append(train_acc)

        # ---- 2. EVALUATE ON VALIDATION SET ----
        val_loss, val_acc = evaluate(model, val_loader, loss_fn)
        val_losses.append(val_loss)
        val_accuracies.append(val_acc)
    
    return train_losses,train_accuracies,val_losses,val_accuracies

# Codegrade Tag Question2e
# Do *not* remove the tag above
trained = train(simple_nn, train_loader, val_loader, num_epochs=10, learning_rate=0.0001)
state_dict = simple_nn.state_dict()
torch.save(state_dict, 'simple_nn_trained.pth')
fig, ax = plt.subplots(1, 1, figsize=(6, 5))
train_losses, train_accuracies, val_losses, val_accuracies = trained
ax.plot(range(1, 11),train_accuracies, label='Train Accuracy')
ax.plot(range(1, 11), val_accuracies, label='Validation Accuracy')
ax.set_xlabel('Epoch')
ax.set_ylabel('Accuracy')
ax.set_title('Training and Validation Accuracy')
ax.legend()
plt.show()


# Codegrade Tag Question3a
# Do *not* remove the tag above

class TwoLayerNN(nn.Module):
    """
    A simple neural network with two fully connected hidden layers,
    each of which is followed by a dropout layer.
    """
    def __init__(self, input_size, hidden_sizes, output_size, dropout_prob):
        """
        Constructor.
        Parameters:
        - input_size: integer, the flattened vector size of inputs
        - hidden_sizes: a tuple of two integers, the sizes of the two hidden layers
        - output_size: number of outputs (number of classes)
        - dropout_prob: float, probability parameter for the dropout layers
        """
        super().__init__()

        m1, m2 = hidden_sizes  # unpack hidden layer sizes

        self.network = nn.Sequential(
            nn.Linear(input_size, m1),
            nn.ReLU(),
            nn.Dropout(dropout_prob),

            nn.Linear(m1, m2),
            nn.ReLU(),
            nn.Dropout(dropout_prob),

            nn.Linear(m2, output_size)
        )

    def forward(self, x):
        """
        Evaluates the network for the given argument x.
        """
        # Flatten input: [batch, 1, 28, 28] -> [batch, 784]
        x = x.view(x.size(0), -1)
        return self.network(x)

two_layer_nn = TwoLayerNN(
    input_size=784,
    hidden_sizes=(512, 256),
    output_size=10,
    dropout_prob=0.2
)

# Codegrade Tag Question4a
# Do *not* remove the tag above

import torch
import torch.nn as nn

class CNN(nn.Module):
    """
    A convolutional neural network with three convolutional layers
    (each contains batch normalization, relu activation, followed by max pooling),
    flattening, followed by three fully connected layers
    (the first two followed by dropouts).
    """

    def __init__(
        self,
        in_sizes,
        in_channels,
        conv1_channels,
        conv2_channels,
        conv3_channels,
        kernel_size,
        padding,
        pool_size,
        fc1_size,
        fc2_size,
        out_size,
        dropout_probability
    ):
        super().__init__()

        self.conv1 = nn.Sequential( # Convolutional part 
            nn.Conv2d(in_channels, conv1_channels, kernel_size=kernel_size, padding=padding),
            nn.BatchNorm2d(conv1_channels),
            nn.ReLU(),
            nn.MaxPool2d(pool_size)
        )

        self.conv2 = nn.Sequential(
            nn.Conv2d(conv1_channels, conv2_channels, kernel_size=kernel_size, padding=padding),
            nn.BatchNorm2d(conv2_channels),
            nn.ReLU(),
            nn.MaxPool2d(pool_size)
        )

        self.conv3 = nn.Sequential(
            nn.Conv2d(conv2_channels, conv3_channels, kernel_size=kernel_size, padding=padding),
            nn.BatchNorm2d(conv3_channels),
            nn.ReLU(),
            nn.MaxPool2d(pool_size)
        )

        with torch.no_grad(): # Compute flattened size dynamically
            dummy = torch.zeros(1, in_channels, in_sizes[1], in_sizes[0])
            dummy = self.conv3(self.conv2(self.conv1(dummy)))
            flattened_size = dummy.view(1, -1).size(1)

        self.fc = nn.Sequential( # Fully connected part
            nn.Linear(flattened_size, fc1_size),
            nn.ReLU(),
            nn.Dropout(dropout_probability),

            nn.Linear(fc1_size, fc2_size),
            nn.ReLU(),
            nn.Dropout(dropout_probability),

            nn.Linear(fc2_size, out_size)
        )

    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        x = self.conv3(x)

        x = torch.flatten(x, start_dim=1)
        x = self.fc(x)
        return x

cnn = CNN(
    in_sizes=(28, 28),
    in_channels=1,
    conv1_channels=32,
    conv2_channels=64,
    conv3_channels=128,
    kernel_size=3,
    padding=1,
    pool_size=2,
    fc1_size=256,
    fc2_size=128,
    out_size=10,
    dropout_probability=0.5
)

