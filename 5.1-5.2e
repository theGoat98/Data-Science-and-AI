# Codegrade Tag Question1
# Do *not* remove the tag above

from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)
val_loader = DataLoader(val_subset, batch_size=64, shuffle=False)
test_loader = DataLoader(test_set, batch_size=64, shuffle=False)

iter_train = iter(train_loader)
images, labels = next(iter_train)
batch_labels = labels  

fig, axs = plt.subplots(2, 5, figsize=(10, 5))
axs = axs.ravel()
for i in range(10):
    axs[i].imshow(images[i].squeeze(), cmap='gray')
    axs[i].set_title(f'Label: {batch_labels[i].item()}', va = "bottom")
    axs[i].axis('off')
plt.tight_layout()
plt.show()

# Codegrade Tag Question2a
# Do *not* remove the tag above

import torch.nn as nn
import torch

class SimpleNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super().__init__()

        # Build the network as required
        self.network = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, output_size)
        )

    def forward(self, x):
        x = x.view(x.size(0), -1)
        return self.network(x)
    
simple_nn = SimpleNN(input_size=784, hidden_size=64, output_size=10)


# Codegrade Tag Question2b
# Do *not* remove the tag above

import torch.optim as optim

def train_one_epoch(model, loader, loss_fn, optimizer):
    model.train() 
    losses = []
    correct = 0
    total = 0
    for data, target in loader: 
        optimizer.zero_grad()
        output = model(data)
        loss = loss_fn(output, target)
        loss.backward()
        optimizer.step()
        
        losses.append(loss.item())  
        _, predicted = torch.max(output, 1)
        total += target.size(0)
        correct += (predicted == target).sum().item()   
    mean_loss = sum(losses) / len(losses)
    accuracy = correct / total

    return mean_loss, accuracy

# Codegrade Tag Question2c
# Do *not* remove the tag above

def evaluate(model, loader, loss_fn):
    model.eval()
    losses = []
    correct = 0
    total = 0
    for data, target in loader: 
        torch.no_grad()
        output = model(data)
        loss = loss_fn(output, target)
        
        losses.append(loss.item())  
        _, predicted = torch.max(output, 1)
        total += target.size(0)
        correct += (predicted == target).sum().item()   
    mean_loss = sum(losses) / len(losses)
    accuracy = correct / total
    return mean_loss, accuracy

# Codegrade Tag Question2d
# Do *not* remove the tag above

def train(model, train_loader, val_loader, num_epochs, learning_rate = 0.0001):
    model.train()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    loss_fn = nn.CrossEntropyLoss()
    train_losses = []
    train_accuracies = []
    val_losses = []
    val_accuracies = []

    for epoch in range(num_epochs):
        train_loss, train_acc = train_one_epoch(model, train_loader, loss_fn, optimizer)
        train_losses.append(train_loss)
        train_accuracies.append(train_acc)

        # ---- 2. EVALUATE ON VALIDATION SET ----
        val_loss, val_acc = evaluate(model, val_loader, loss_fn)
        val_losses.append(val_loss)
        val_accuracies.append(val_acc)
    
    return train_losses,train_accuracies,val_losses,val_accuracies

# Codegrade Tag Question2e
# Do *not* remove the tag above
trained = train(simple_nn, train_loader, val_loader, num_epochs=10, learning_rate=0.0001)
state_dict = simple_nn.state_dict()
torch.save(state_dict, 'simple_nn_trained.pth')
fig, ax = plt.subplots(1, 1, figsize=(6, 5))
train_losses, train_accuracies, val_losses, val_accuracies = trained
ax.plot(range(1, 11),train_accuracies, label='Train Accuracy')
ax.plot(range(1, 11), val_accuracies, label='Validation Accuracy')
ax.set_xlabel('Epoch')
ax.set_ylabel('Accuracy')
ax.set_title('Training and Validation Accuracy')
ax.legend()
plt.show()


# Codegrade Tag Question3a
# Do *not* remove the tag above

class TwoLayerNN(nn.Module):
    """
    A simple neural network with two fully connected hidden layers,
    each of which is followed by a dropout layer.
    """
    def __init__(self, input_size, hidden_sizes, output_size, dropout_prob):
        """
        Constructor.
        Parameters:
        - input_size: integer, the flattened vector size of inputs
        - hidden_sizes: a tuple of two integers, the sizes of the two hidden layers
        - output_size: number of outputs (number of classes)
        - dropout_prob: float, probability parameter for the dropout layers
        """
        super().__init__()

        m1, m2 = hidden_sizes  # unpack hidden layer sizes

        self.network = nn.Sequential(
            nn.Linear(input_size, m1),
            nn.ReLU(),
            nn.Dropout(dropout_prob),

            nn.Linear(m1, m2),
            nn.ReLU(),
            nn.Dropout(dropout_prob),

            nn.Linear(m2, output_size)
        )

    def forward(self, x):
        """
        Evaluates the network for the given argument x.
        """
        # Flatten input: [batch, 1, 28, 28] -> [batch, 784]
        x = x.view(x.size(0), -1)
        return self.network(x)

two_layer_nn = TwoLayerNN(
    input_size=784,
    hidden_sizes=(512, 256),
    output_size=10,
    dropout_prob=0.2
)

